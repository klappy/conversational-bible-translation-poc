# ğŸ§ª Bible Translation Assistant - Automated Testing

## Overview

This testing framework provides intelligent, conversational testing of the Bible Translation Assistant. Instead of rigid scripts, our AI-powered tests actually **converse naturally** with the translation assistant, simulating real workshop attendees.

## ğŸ­ Intelligent Testing Approach

Our tests don't just send pre-scripted messages. They:
- Read and understand AI responses
- Ask clarifying questions when appropriate
- Use suggestions intelligently (not always)
- Simulate different user personas
- Complete full translation workflows naturally
- Verify the system works end-to-end

## Test Personas

### 1. **Maria** - Curious Beginner 
- First-time Bible translator
- Spanish speaker translating for youth group
- Asks questions, needs clarification
- Engaging and curious

### 2. **John** - Experienced Translator
- Knows what he wants
- Direct and efficient
- Uses suggestions often
- Translating Greek to French

### 3. **Sarah** - Confused User
- Uncertain about the process
- Needs guidance
- Sometimes changes mind
- Translating for young children

## Running Tests

### Test All Personas (Workshop Simulation)
```bash
# Test locally
npm run test:workshop

# Test production
npm run test:workshop:prod
```

### Test Individual Personas
```bash
# Test beginner persona
npm run test:beginner

# Test experienced translator
npm run test:expert  

# Test confused user
npm run test:confused
```

### Basic API Tests
```bash
# Run all API tests
npm test

# Test conversation flow
npm run test:flow

# Test agent behaviors
npm run test:agents
```

## What Gets Tested

### Full Conversation Flow
âœ… Initial greeting and response
âœ… Language customization (7 settings)
âœ… Phase transitions (Planning â†’ Understanding â†’ Drafting)
âœ… Scripture presentation and comprehension
âœ… Draft creation and refinement
âœ… State persistence throughout
âœ… Agent coordination (who responds when)

### Natural Variations
âœ… Using suggestions vs typing own responses
âœ… Asking clarifying questions
âœ… Providing detailed vs brief answers
âœ… Confusion and corrections
âœ… Different translation approaches

### State Management
âœ… Settings are saved correctly
âœ… Progress persists between messages
âœ… Phase transitions work properly
âœ… Canvas state updates appropriately

## Example Test Output

```
ğŸ­ Workshop Attendee: Maria
ğŸ“ Background: First-time Bible translator, Spanish speaker
============================================================

ğŸ‘¤ Maria: "Hello! I'm interested in translating the Bible for my youth group"

ğŸ“– Translation Assistant: "Welcome! I'm excited to help you translate..."

ğŸ‘¤ Maria: "I'd like to customize the reading level and style"

ğŸ“– Translation Assistant: "Great! Let's customize your settings. First, what language..."

ğŸ‘¤ Maria: "English"

ğŸ“ Canvas Scribe: "Noted!"

ğŸ“– Translation Assistant: "Perfect! What language are we translating from?"

ğŸ‘¤ Maria: "Hebrew. I want to work from the original text."

[... conversation continues naturally ...]

ğŸ“Š TEST REPORT
============================================================
Persona: Maria
Exchanges: 12
Final Phase: drafting
Style Guide Complete: true
Has Draft: true
Success: âœ…
```

## How It Works

1. **Persona Selection**: Each test run selects a persona with specific characteristics
2. **Natural Conversation**: The test reads AI responses and responds appropriately
3. **Decision Making**: Tests decide whether to use suggestions or type custom responses
4. **Context Awareness**: Tests understand what phase they're in and respond accordingly
5. **Validation**: Tests verify that settings are saved and progress is made

## Adding New Test Scenarios

To add a new persona, edit `intelligent-conversation-tester.js`:

```javascript
workshop_leader: {
  name: "Pastor David",
  background: "Leading a translation workshop",
  style: "organized, methodical",
  preferences: {
    conversationLanguage: "English",
    sourceLanguage: "Hebrew",
    targetLanguage: "Swahili",
    // ... etc
  }
}
```

## Success Criteria

A test is considered successful if:
- âœ… All 7 style guide settings are collected
- âœ… The conversation progresses through phases naturally
- âœ… A draft translation is produced (or attempted)
- âœ… State persists throughout the conversation
- âœ… No errors occur during the flow

## Continuous Integration

These tests can be run in CI/CD pipelines:

```yaml
# Example GitHub Actions
- name: Start local server
  run: npm run dev:netlify &
  
- name: Wait for server
  run: sleep 10
  
- name: Run workshop simulation
  run: npm run test:workshop
```

## Debugging Failed Tests

If a test fails:

1. **Check the conversation log** - See exactly what was said
2. **Verify state changes** - Ensure settings are being saved
3. **Check agent responses** - Verify all agents are responding appropriately
4. **Review phase transitions** - Ensure workflow progresses correctly

## Benefits of This Approach

### ğŸ¯ Realistic Testing
- Tests interact like real users would
- Natural language variations
- Realistic confusion and clarification

### ğŸ”„ Comprehensive Coverage
- Multiple personas test different paths
- Edge cases are naturally discovered
- Full end-to-end validation

### ğŸ›¡ï¸ Confidence in Changes
- Run before deploying to ensure nothing breaks
- Catch issues that rigid tests might miss
- Verify the complete user experience works

### ğŸš€ Easy to Extend
- Add new personas easily
- Test new features naturally
- No need to rewrite rigid test scripts

## Tips

1. **Run tests after major changes** to ensure nothing broke
2. **Use individual persona tests** for quick checks
3. **Run full workshop simulation** before deployments
4. **Check both local and production** environments
5. **Review conversation logs** to understand failures

---

*"These tests don't just verify the code works - they verify that real people can successfully use the system!"*
